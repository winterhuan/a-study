# æ¯æ—¥è‹±è¯­å­¦ä¹  - 2025-09-30

## ğŸ“š ä»Šæ—¥è¯æ±‡ (10ä¸ª)

### 1. semaphore /ËˆsemÉ™fÉ”Ë(r)/ ğŸŸ¡ Concurrency

**å®šä¹‰**: A synchronization primitive that controls access to shared resources through counters

**é¡¹ç›®è¯­å¢ƒ**:
- "Concurrency: asynchronous, mutex, semaphore, deadlock" (CLAUDE.md)
- åœ¨å¹¶å‘ç¼–ç¨‹ä¸­,semaphoreæ˜¯æ§åˆ¶å¤šä¸ªçº¿ç¨‹è®¿é—®æœ‰é™èµ„æºçš„åŒæ­¥æœºåˆ¶
- "counting semaphores in concurrent programming" - è®¡æ•°ä¿¡å·é‡å…è®¸Nä¸ªçº¿ç¨‹åŒæ—¶è®¿é—®

**æŠ€æœ¯è§£é‡Š**:
A semaphore is a synchronization primitive that maintains a counter to control access to shared resources. Unlike a mutex (which is binary), a semaphore allows multiple threads to access the resource simultaneously up to a specified limit.

ä¿¡å·é‡æ˜¯ä¸€ä¸ªç»´æŠ¤è®¡æ•°å™¨çš„åŒæ­¥åŸè¯­,ç”¨äºæ§åˆ¶å¯¹å…±äº«èµ„æºçš„è®¿é—®ã€‚ä¸äº’æ–¥é”(äºŒå…ƒ)ä¸åŒ,ä¿¡å·é‡å…è®¸å¤šä¸ªçº¿ç¨‹åŒæ—¶è®¿é—®èµ„æº,ç›´åˆ°è¾¾åˆ°æŒ‡å®šé™åˆ¶ã€‚

**è®°å¿†æŠ€å·§**:
- è¯æº: semaphore = sema(ä¿¡å·) + phore(æºå¸¦è€…)
- è”æƒ³: Think of a parking lot with N spaces - semaphore is the counter tracking available spots
- ç›¸å…³è¯: mutex, lock, synchronization, resource pool, concurrent

**ä¾‹å¥**:
1. We use a semaphore to limit concurrent API calls to 100 requests per second.
2. The tokio::sync::Semaphore ensures only 5 threads can process files simultaneously.
3. Semaphore-based rate limiting prevents overwhelming downstream services.

**ç»ƒä¹ **:
1. [å¡«ç©º] A _______ can control access for multiple threads, while a mutex allows only one.
2. [ç¿»è¯‘] è¯·ç”¨è‹±è¯­è§£é‡Šä¿¡å·é‡å’Œäº’æ–¥é”çš„ä¸»è¦åŒºåˆ«
3. [åº”ç”¨] åœ¨ä»€ä¹ˆåœºæ™¯ä¸‹ä½¿ç”¨semaphoreæ¯”mutexæ›´åˆé€‚?

---

### 2. mutex /ËˆmjuËteks/ ğŸŸ¡ Concurrency

**å®šä¹‰**: Mutual exclusion lock ensuring only one thread accesses a resource at a time

**é¡¹ç›®è¯­å¢ƒ**:
- "Concurrency: asynchronous, mutex, semaphore, deadlock" (CLAUDE.md)
- mutexæ˜¯å¹¶å‘ç¼–ç¨‹çš„åŸºç¡€åŒæ­¥åŸè¯­
- "Rust std::sync::Mutex" - Rustæä¾›çº¿ç¨‹å®‰å…¨çš„äº’æ–¥é”å®ç°

**æŠ€æœ¯è§£é‡Š**:
A mutex (mutual exclusion) is a binary synchronization primitive that ensures exclusive access to a shared resource. Only one thread can hold the lock at a time. In Rust, std::sync::Mutex provides thread-safe access with compile-time guarantees.

äº’æ–¥é”æ˜¯ä¸€ä¸ªäºŒå…ƒåŒæ­¥åŸè¯­,ç¡®ä¿å¯¹å…±äº«èµ„æºçš„ç‹¬å è®¿é—®ã€‚åŒä¸€æ—¶åˆ»åªæœ‰ä¸€ä¸ªçº¿ç¨‹å¯ä»¥æŒæœ‰é”ã€‚åœ¨Rustä¸­,std::sync::Mutexæä¾›çº¿ç¨‹å®‰å…¨è®¿é—®å’Œç¼–è¯‘æ—¶ä¿è¯ã€‚

**è®°å¿†æŠ€å·§**:
- è¯æº: mutex = MUTual EXclusion, äº’æ–¥çš„ç¼©å†™
- è”æƒ³: Think of a single-stall bathroom with a lock - only one person can use it at a time
- ç›¸å…³è¯: semaphore, lock, critical section, race condition, thread-safe

**ä¾‹å¥**:
1. The Rust std::sync::Mutex ensures thread-safe access to the shared counter.
2. Always acquire the mutex before modifying shared state to prevent race conditions.
3. If a thread panics while holding a mutex, the mutex becomes poisoned in Rust.

**ç»ƒä¹ **:
1. [å¡«ç©º] Use a _______ to ensure only one thread can modify the shared counter at a time.
2. [ç¿»è¯‘] è¯·ç”¨è‹±è¯­è§£é‡Šä¸ºä»€ä¹ˆéœ€è¦ä½¿ç”¨mutexæ¥ä¿æŠ¤å…±äº«å˜é‡
3. [åº”ç”¨] æè¿°Rustä¸­mutexçš„æ‰€æœ‰æƒæœºåˆ¶å¦‚ä½•é˜²æ­¢æ•°æ®ç«äº‰

---

### 3. deadlock /ËˆdedlÉ’k/ ğŸŸ¡ Concurrency

**å®šä¹‰**: A state where processes wait indefinitely for each other's resources, causing system freeze

**é¡¹ç›®è¯­å¢ƒ**:
- "Concurrency: asynchronous, mutex, semaphore, deadlock" (CLAUDE.md)
- deadlockæ˜¯å¹¶å‘ç¼–ç¨‹ä¸­å¿…é¡»é¿å…çš„ä¸¥é‡é—®é¢˜
- "deadlock detection algorithms" - æ£€æµ‹å’Œé¢„é˜²æ­»é”çš„ç®—æ³•

**æŠ€æœ¯è§£é‡Š**:
Deadlock occurs when two or more processes wait indefinitely for resources held by each other, creating a circular dependency. The four necessary conditions are: mutual exclusion, hold and wait, no preemption, and circular wait.

æ­»é”å‘ç”Ÿåœ¨ä¸¤ä¸ªæˆ–å¤šä¸ªè¿›ç¨‹æ— é™æœŸåœ°ç­‰å¾…å¯¹æ–¹æŒæœ‰çš„èµ„æº,å½¢æˆå¾ªç¯ä¾èµ–ã€‚å››ä¸ªå¿…è¦æ¡ä»¶æ˜¯:äº’æ–¥ã€æŒæœ‰å¹¶ç­‰å¾…ã€ä¸å¯æŠ¢å å’Œå¾ªç¯ç­‰å¾…ã€‚

**è®°å¿†æŠ€å·§**:
- è¯æº: deadlock = dead(æ­») + lock(é”), ç³»ç»Ÿè¢«é”æ­»
- è”æƒ³: Two cars at a narrow bridge, each waiting for the other to back up - neither can proceed
- ç›¸å…³è¯: mutex, race condition, livelock, starvation, circular wait

**ä¾‹å¥**:
1. The system experienced a deadlock when two threads tried to acquire locks in opposite orders.
2. Flink's scheduler implements deadlock prevention through careful resource allocation.
3. We detected a deadlock in the database using timeout mechanisms.

**ç»ƒä¹ **:
1. [å¡«ç©º] A _______ occurs when two threads wait indefinitely for resources held by each other.
2. [ç¿»è¯‘] è¯·ç”¨è‹±è¯­è§£é‡Šæ­»é”çš„å››ä¸ªå¿…è¦æ¡ä»¶
3. [åº”ç”¨] æè¿°ä¸€ç§åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­é¢„é˜²æ­»é”çš„ç­–ç•¥

---

### 4. throughput /ËˆÎ¸ruËpÊŠt/ ğŸŸ¡ Stream Processing

**å®šä¹‰**: The amount of data or operations processed successfully in a given time period

**é¡¹ç›®è¯­å¢ƒ**:
- "Stream Processing: watermark, windowing, backpressure, throughput" (CLAUDE.md)
- throughputæ˜¯è¡¡é‡æµå¤„ç†ç³»ç»Ÿæ€§èƒ½çš„æ ¸å¿ƒæŒ‡æ ‡
- "Increase parallelism for better throughput" - é€šè¿‡æé«˜å¹¶è¡Œåº¦æ¥æå‡ååé‡

**æŠ€æœ¯è§£é‡Š**:
Throughput measures the rate at which a system processes data, typically expressed as records/second or bytes/second. In Flink and Spark, throughput is a critical performance metric that depends on parallelism and resource allocation.

ååé‡è¡¡é‡ç³»ç»Ÿå¤„ç†æ•°æ®çš„é€Ÿç‡,é€šå¸¸ä»¥è®°å½•æ•°/ç§’æˆ–å­—èŠ‚æ•°/ç§’è¡¨ç¤ºã€‚åœ¨Flinkå’ŒSparkä¸­,ååé‡æ˜¯å…³é”®æ€§èƒ½æŒ‡æ ‡,å–å†³äºå¹¶è¡Œåº¦å’Œèµ„æºåˆ†é…ã€‚

**è®°å¿†æŠ€å·§**:
- è¯æº: throughput = through(é€šè¿‡) + put(æ”¾ç½®), æ•°æ®é€šè¿‡ç³»ç»Ÿçš„é€Ÿç‡
- è”æƒ³: Think of a highway - throughput is how many cars pass through per hour
- ç›¸å…³è¯: latency, bandwidth, parallelism, backpressure, performance

**ä¾‹å¥**:
1. Our Flink job processes data with a throughput of 100,000 events per second.
2. Increasing parallelism from 4 to 16 doubled the system's throughput.
3. Monitor throughput metrics to detect performance degradation in real-time pipelines.

**ç»ƒä¹ **:
1. [å¡«ç©º] We increased the _______ from 10,000 to 50,000 records per second by adding parallel tasks.
2. [ç¿»è¯‘] è¯·ç”¨è‹±è¯­è§£é‡Šååé‡å’Œå»¶è¿Ÿçš„æƒè¡¡å…³ç³»
3. [åº”ç”¨] æè¿°å¦‚ä½•åœ¨Flinkä½œä¸šä¸­æé«˜throughput

---

### 5. idempotent /ËŒaÉªdÉªmËˆpÉ™ÊŠtÉ™nt/ ğŸ”´ Architecture

**å®šä¹‰**: An operation that produces the same result when executed multiple times with the same input

**é¡¹ç›®è¯­å¢ƒ**:
- "Architecture: orchestration, scalability, resilience, idempotent" (CLAUDE.md)
- idempotentæ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡çš„æ ¸å¿ƒåŸåˆ™
- "designing idempotent APIs" - è®¾è®¡å¹‚ç­‰APIæ˜¯æœ€ä½³å®è·µ

**æŠ€æœ¯è§£é‡Š**:
An idempotent operation guarantees the same result regardless of how many times it's executed. For example, 'SET x = 5' is idempotent, while 'ADD x += 5' is not. Idempotency enables safe retries without side effects.

å¹‚ç­‰æ“ä½œä¿è¯æ— è®ºæ‰§è¡Œå¤šå°‘æ¬¡ç»“æœéƒ½ç›¸åŒã€‚ä¾‹å¦‚,'SET x = 5'æ˜¯å¹‚ç­‰çš„,è€Œ'ADD x += 5'ä¸æ˜¯ã€‚å¹‚ç­‰æ€§ä½¿å¾—å®‰å…¨é‡è¯•æˆä¸ºå¯èƒ½ã€‚

**è®°å¿†æŠ€å·§**:
- è¯æº: idempotent = idem(ç›¸åŒ,æ‹‰ä¸è¯­) + potent(æœ‰æ•ˆåŠ›çš„)
- è”æƒ³: A light switch already ON - pressing it again keeps it ON (idempotent)
- ç›¸å…³è¯: stateless, deterministic, pure function, side-effect-free, retry-safe

**ä¾‹å¥**:
1. Design your REST APIs to be idempotent so clients can safely retry failed requests.
2. Flink's exactly-once semantics rely on idempotent sinks that handle duplicate writes.
3. An idempotent operation like 'SET x = 5' can be retried without side effects.

**ç»ƒä¹ **:
1. [å¡«ç©º] An _______ operation produces the same result no matter how many times it's executed.
2. [ç¿»è¯‘] è¯·ç”¨è‹±è¯­è§£é‡Šä¸ºä»€ä¹ˆå¹‚ç­‰æ€§å¯¹åˆ†å¸ƒå¼ç³»ç»Ÿå¾ˆé‡è¦
3. [åº”ç”¨] ä¸¾ä¾‹è¯´æ˜å¹‚ç­‰å’Œéå¹‚ç­‰æ“ä½œçš„åŒºåˆ«

---

### 6. consensus /kÉ™nËˆsensÉ™s/ ğŸ”´ Distributed Systems

**å®šä¹‰**: An agreement protocol among distributed nodes on a single value or system state

**é¡¹ç›®è¯­å¢ƒ**:
- "Distributed Systems: executor, checkpoint, partition, consensus" (CLAUDE.md)
- consensusæ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿçš„æ ¸å¿ƒç®—æ³•
- "Paxos and Raft consensus" - è‘—åçš„å…±è¯†ç®—æ³•

**æŠ€æœ¯è§£é‡Š**:
Consensus is a fundamental problem where multiple nodes must agree on a single value despite failures. Famous algorithms include Paxos, Raft, and ZAB. Consensus ensures consistency and fault tolerance in distributed systems.

å…±è¯†æ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„åŸºæœ¬é—®é¢˜,å¤šä¸ªèŠ‚ç‚¹å¿…é¡»åœ¨å­˜åœ¨æ•…éšœçš„æƒ…å†µä¸‹å°±å•ä¸ªå€¼è¾¾æˆä¸€è‡´ã€‚è‘—åç®—æ³•åŒ…æ‹¬Paxosã€Raftå’ŒZABã€‚å…±è¯†ç¡®ä¿ä¸€è‡´æ€§å’Œå®¹é”™ã€‚

**è®°å¿†æŠ€å·§**:
- è¯æº: consensus = con(å…±åŒ) + sensus(æ„è§), å…±åŒçš„æ„è§
- è”æƒ³: Think of a jury reaching unanimous verdict - all must agree
- ç›¸å…³è¯: coordination, replication, quorum, leader election, fault tolerance

**ä¾‹å¥**:
1. Raft consensus algorithm ensures all nodes agree on the current leader.
2. ZooKeeper uses ZAB consensus to maintain consistency across replicas.
3. Achieving consensus is challenging due to network partitions and node failures.

**ç»ƒä¹ **:
1. [å¡«ç©º] ZooKeeper uses the ZAB _______ algorithm to ensure all nodes agree.
2. [ç¿»è¯‘] è¯·ç”¨è‹±è¯­è§£é‡Šä¸ºä»€ä¹ˆåˆ†å¸ƒå¼ç³»ç»Ÿéœ€è¦å…±è¯†ç®—æ³•
3. [åº”ç”¨] æ¯”è¾ƒPaxoså’ŒRaftå…±è¯†ç®—æ³•çš„ç‰¹ç‚¹

---

### 7. middleware /ËˆmÉªdlweÉ™(r)/ ğŸŸ¡ Architecture

**å®šä¹‰**: Software layer between application and underlying services, providing common functionalities

**é¡¹ç›®è¯­å¢ƒ**:
- "message middleware like Kafka" (lifeOS/english/vocabulary/tech-terms.json)
- middlewareå¤„ç†æ¨ªåˆ‡å…³æ³¨ç‚¹å¦‚æ—¥å¿—ã€ç›‘æ§ã€è®¤è¯
- "request-response middleware chain" - ä¸­é—´ä»¶é“¾å¤„ç†è¯·æ±‚

**æŠ€æœ¯è§£é‡Š**:
Middleware provides common services beyond what's offered by the OS. It sits between applications and infrastructure, handling messaging (Kafka), caching (Redis), authentication, and logging. Enables loose coupling and separation of concerns.

ä¸­é—´ä»¶æä¾›è¶…å‡ºæ“ä½œç³»ç»Ÿçš„é€šç”¨æœåŠ¡ã€‚å®ƒä½äºåº”ç”¨ç¨‹åºå’ŒåŸºç¡€è®¾æ–½ä¹‹é—´,å¤„ç†æ¶ˆæ¯ä¼ é€’(Kafka)ã€ç¼“å­˜(Redis)ã€è®¤è¯å’Œæ—¥å¿—ã€‚å®ç°æ¾è€¦åˆå’Œå…³æ³¨ç‚¹åˆ†ç¦»ã€‚

**è®°å¿†æŠ€å·§**:
- è¯æº: middleware = middle(ä¸­é—´) + ware(è½¯ä»¶)
- è”æƒ³: A waiter between customers (apps) and kitchen (infrastructure)
- ç›¸å…³è¯: message broker, service layer, API gateway, proxy, adapter

**ä¾‹å¥**:
1. We use Kafka as messaging middleware to enable async communication between services.
2. The authentication middleware validates JWT tokens before requests reach endpoints.
3. Middleware components like Redis and Kafka are essential for scalable microservices.

**ç»ƒä¹ **:
1. [å¡«ç©º] Kafka acts as _______ for decoupling producers and consumers.
2. [ç¿»è¯‘] è¯·ç”¨è‹±è¯­è§£é‡Šä¸­é—´ä»¶åœ¨å¾®æœåŠ¡æ¶æ„ä¸­çš„ä½œç”¨
3. [åº”ç”¨] åˆ—ä¸¾ä¸åŒç±»å‹çš„middlewareåŠå…¶ç”¨é€”

---

### 8. sharding /ËˆÊƒÉ‘ËdÉªÅ‹/ ğŸ”´ Distributed Systems

**å®šä¹‰**: Horizontally partitioning data across multiple database instances to improve scalability

**é¡¹ç›®è¯­å¢ƒ**:
- "Distributed Systems: replication, sharding" (CLAUDE.md)
- shardingé€šè¿‡æ°´å¹³åˆ†åŒºæé«˜å¯æ‰©å±•æ€§
- "database sharding strategies" - æ•°æ®åº“åˆ†ç‰‡ç­–ç•¥

**æŠ€æœ¯è§£é‡Š**:
Sharding horizontally partitions data across multiple independent database instances. Each shard contains a subset determined by a shard key. Enables horizontal scalability but introduces complexity in query routing and consistency.

åˆ†ç‰‡å°†æ•°æ®æ°´å¹³åˆ†åŒºåˆ°å¤šä¸ªç‹¬ç«‹æ•°æ®åº“å®ä¾‹ã€‚æ¯ä¸ªåˆ†ç‰‡åŒ…å«ç”±åˆ†ç‰‡é”®å†³å®šçš„å­é›†ã€‚å®ç°æ°´å¹³å¯æ‰©å±•æ€§ä½†åœ¨æŸ¥è¯¢è·¯ç”±å’Œä¸€è‡´æ€§æ–¹é¢å¼•å…¥å¤æ‚æ€§ã€‚

**è®°å¿†æŠ€å·§**:
- è¯æº: sharding = shard(ç¢ç‰‡) + ing, å°†æ•°æ®æ‰“ç¢åˆ†å¸ƒ
- è”æƒ³: Breaking a large pizza into slices (shards) for different people (servers)
- ç›¸å…³è¯: partitioning, horizontal scaling, consistent hashing, replication

**ä¾‹å¥**:
1. Our MongoDB cluster uses sharding to distribute 100TB across 50 shards.
2. Choosing the right shard key is critical - poor choices lead to unbalanced load.
3. Sharding enables horizontal scalability but complicates cross-shard queries.

**ç»ƒä¹ **:
1. [å¡«ç©º] We use _______ to horizontally partition our database across 10 servers.
2. [ç¿»è¯‘] è¯·ç”¨è‹±è¯­è§£é‡Šåˆ†ç‰‡å’Œå‰¯æœ¬çš„åŒºåˆ«
3. [åº”ç”¨] æè¿°å®ç°æ•°æ®åº“åˆ†ç‰‡æ—¶é¢ä¸´çš„æŒ‘æˆ˜

---

### 9. compaction /kÉ™mËˆpÃ¦kÊƒn/ ğŸ”´ Database

**å®šä¹‰**: Process of consolidating and optimizing stored data by removing redundancies

**é¡¹ç›®è¯­å¢ƒ**:
- "Database: columnar, compaction, indexing" (CLAUDE.md)
- compactionæ˜¯æ•°æ®åº“ä¼˜åŒ–çš„å…³é”®è¿‡ç¨‹
- "log-structured merge compaction" - LSMæ ‘å‹ç¼©æœºåˆ¶

**æŠ€æœ¯è§£é‡Š**:
Compaction consolidates data files, removes deleted records, and optimizes storage. In LSM-tree databases (RocksDB, Cassandra), compaction merges SSTables to reduce read amplification. Types include minor and major compaction.

å‹ç¼©åˆå¹¶æ•°æ®æ–‡ä»¶ã€åˆ é™¤å·²åˆ é™¤è®°å½•å¹¶ä¼˜åŒ–å­˜å‚¨ã€‚åœ¨LSMæ ‘æ•°æ®åº“(RocksDBã€Cassandra)ä¸­,å‹ç¼©åˆå¹¶SSTableä»¥å‡å°‘è¯»æ”¾å¤§ã€‚ç±»å‹åŒ…æ‹¬minorå’Œmajorå‹ç¼©ã€‚

**è®°å¿†æŠ€å·§**:
- è¯æº: compaction = compact(ç´§å‡‘) + ion, ä½¿æ•°æ®ç´§å‡‘
- è”æƒ³: Compacting trash - squeeze out air and consolidate into smaller space
- ç›¸å…³è¯: LSM-tree, SSTable, merge, garbage collection, defragmentation

**ä¾‹å¥**:
1. Cassandra automatically triggers compaction to merge SSTables and reclaim space.
2. Tuning compaction strategies in RocksDB balances write and read performance.
3. Major compaction can increase I/O load but significantly improves query speed.

**ç»ƒä¹ **:
1. [å¡«ç©º] RocksDB performs _______ to merge multiple SSTables and reduce read amplification.
2. [ç¿»è¯‘] è¯·ç”¨è‹±è¯­è§£é‡Šä¸ºä»€ä¹ˆLSMæ ‘æ•°æ®åº“éœ€è¦compaction
3. [åº”ç”¨] æ¯”è¾ƒminor compactionå’Œmajor compaction

---

### 10. embedding /ÉªmËˆbedÉªÅ‹/ ğŸ”´ Machine Learning

**å®šä¹‰**: Dense vector representation of data in continuous space, capturing semantic relationships

**é¡¹ç›®è¯­å¢ƒ**:
- "text embeddings for semantic search" (lifeOS/english/vocabulary/tech-terms.json)
- embeddingæ˜¯è¯­ä¹‰æœç´¢å’ŒNLPçš„åŸºç¡€
- "storing embeddings in Lance vector database" - å‘é‡æ•°æ®åº“å­˜å‚¨embedding

**æŠ€æœ¯è§£é‡Š**:
Embeddings map discrete objects (words, images) into continuous vector space where semantic similarity is reflected by proximity. Word embeddings (Word2Vec) and contextual embeddings (BERT) enable semantic search and recommendations.

åµŒå…¥å°†ç¦»æ•£å¯¹è±¡(è¯è¯­ã€å›¾åƒ)æ˜ å°„åˆ°è¿ç»­å‘é‡ç©ºé—´,å…¶ä¸­è¯­ä¹‰ç›¸ä¼¼æ€§é€šè¿‡æ¥è¿‘åº¦åæ˜ ã€‚è¯åµŒå…¥(Word2Vec)å’Œä¸Šä¸‹æ–‡åµŒå…¥(BERT)å®ç°è¯­ä¹‰æœç´¢å’Œæ¨èã€‚

**è®°å¿†æŠ€å·§**:
- è¯æº: embedding = embed(åµŒå…¥) + ing, åµŒå…¥åˆ°å‘é‡ç©ºé—´
- è”æƒ³: GPS coordinates placing words on a map where similar items are nearby
- ç›¸å…³è¯: vector, representation learning, semantic similarity, word2vec, transformer

**ä¾‹å¥**:
1. Our RAG system stores document embeddings in Lance for similarity search.
2. Word2Vec embeddings capture relationships like 'king - man + woman = queen'.
3. BERT generates contextual embeddings that vary based on sentence context.

**ç»ƒä¹ **:
1. [å¡«ç©º] We use BERT to generate sentence _______ for semantic search.
2. [ç¿»è¯‘] è¯·ç”¨è‹±è¯­è§£é‡Šembeddingå¦‚ä½•å®ç°è¯­ä¹‰æœç´¢
3. [åº”ç”¨] è§£é‡Šè¯åµŒå…¥å’Œä¸Šä¸‹æ–‡åµŒå…¥çš„åŒºåˆ«

---

## ğŸ“Š å­¦ä¹ ç»Ÿè®¡

- **å­¦ä¹ æ—¶é•¿**: 45åˆ†é’Ÿ (é¢„è®¡)
- **æ–°å­¦è¯æ±‡**: 10ä¸ª
- **é¢†åŸŸåˆ†å¸ƒ**:
  - ğŸŸ¡ Concurrency (å¹¶å‘): 3ä¸ª (semaphore, mutex, deadlock)
  - ğŸŸ¡ Stream Processing (æµå¤„ç†): 1ä¸ª (throughput)
  - ğŸ”´ Architecture (æ¶æ„): 2ä¸ª (idempotent, middleware)
  - ğŸ”´ Distributed Systems (åˆ†å¸ƒå¼): 2ä¸ª (consensus, sharding)
  - ğŸ”´ Database (æ•°æ®åº“): 1ä¸ª (compaction)
  - ğŸ”´ Machine Learning (æœºå™¨å­¦ä¹ ): 1ä¸ª (embedding)
- **éš¾åº¦åˆ†å¸ƒ**:
  - ğŸŸ¡ Intermediate (ä¸­çº§): 6ä¸ª
  - ğŸ”´ Advanced (é«˜çº§): 4ä¸ª

## ğŸ¯ å­¦ä¹ å»ºè®®

1. **ä¼˜å…ˆæŒæ¡å¹¶å‘ä¸‰å…„å¼Ÿ**: semaphore, mutex, deadlock æ˜¯Rustå’Œç³»ç»Ÿç¼–ç¨‹çš„æ ¸å¿ƒæ¦‚å¿µ,å»ºè®®ä»Šå¤©é‡ç‚¹ç†è§£å®ƒä»¬çš„åŒºåˆ«å’Œä½¿ç”¨åœºæ™¯

2. **å…³è”å­¦ä¹ **:
   - å¹¶å‘ç»„: semaphore â†” mutex â†” deadlock (ä¸‰è€…ç´§å¯†ç›¸å…³)
   - åˆ†å¸ƒå¼ç»„: consensus â†” sharding â†” idempotent (ä¸€èµ·æ„å»ºå¯é ç³»ç»Ÿ)
   - å­˜å‚¨ç»„: compaction â†” embedding (æ•°æ®åº“å’ŒMLäº¤å‰)

3. **å®è·µåº”ç”¨**:
   - åœ¨é˜…è¯» Flink/Spark æºç æ—¶ä¸»åŠ¨å¯»æ‰¾è¿™äº›æœ¯è¯­
   - å°è¯•åœ¨Rustä»£ç ä¸­ä½¿ç”¨ std::sync::Mutex å’Œ tokio::sync::Semaphore
   - æŸ¥çœ‹Lanceæ•°æ®åº“å¦‚ä½•å­˜å‚¨å’ŒæŸ¥è¯¢embeddingå‘é‡

4. **é—´éš”å¤ä¹ **:
   - æ˜å¤©(2025-10-01)é‡ç‚¹å¤ä¹ : semaphore, mutex, deadlock
   - åå¤©(2025-10-02)é‡ç‚¹å¤ä¹ : throughput, idempotent, consensus
   - ç¬¬ä¸‰å¤©é‡ç‚¹å¤ä¹ é«˜çº§æ¦‚å¿µ: sharding, compaction, embedding

## âœ… ä»Šæ—¥ä»»åŠ¡

- [x] å­¦ä¹ 10ä¸ªæ–°è¯æ±‡ âœ“
- [ ] å®Œæˆ3ä¸ªç»ƒä¹ é¢˜ (æ¯ä¸ªè¯è‡³å°‘åš1é¢˜)
- [ ] é˜…è¯»ç›¸å…³ä»£ç ç¤ºä¾‹ (åœ¨é¡¹ç›®ä¸­æœç´¢è¿™äº›æœ¯è¯­)
- [ ] ç”¨è‹±è¯­å†™ä¸€æ®µåŒ…å«3ä¸ªæ–°è¯çš„æŠ€æœ¯è§£é‡Š

## ğŸ”„ æ˜æ—¥é¢„å‘Š

æ ¹æ®å­¦ä¹ è®¡åˆ’,æ˜å¤©å°†å­¦ä¹ :
- **é«˜é¢‘æ ¸å¿ƒè¯**: executor, checkpoint, partition, streaming
- **Rustä¸“é¢˜**: trait, borrow, closure
- **æµå¤„ç†æ·±å…¥**: watermark, windowing, backpressure

## ğŸ’ª å­¦ä¹ å¿ƒå¾—

ä»Šå¤©æ˜¯è‹±è¯­å­¦ä¹ ç³»ç»Ÿçš„ç¬¬ä¸€å¤©!é€‰æ‹©çš„10ä¸ªè¯æ±‡è¦†ç›–äº†å¹¶å‘ç¼–ç¨‹ã€åˆ†å¸ƒå¼ç³»ç»Ÿã€æ¶æ„è®¾è®¡ç­‰æ ¸å¿ƒé¢†åŸŸ,éƒ½æ˜¯å®é™…é¡¹ç›®ä¸­é¢‘ç¹å‡ºç°çš„æœ¯è¯­ã€‚

**é‡ç‚¹æ”¶è·**:
- å¹¶å‘ä¸‰å‰‘å®¢ (semaphore/mutex/deadlock) æ„æˆäº†ç†è§£å¤šçº¿ç¨‹ç¼–ç¨‹çš„åŸºç¡€
- åˆ†å¸ƒå¼ç³»ç»Ÿçš„æ ¸å¿ƒ (consensus/sharding/idempotent) æ˜¯æ„å»ºå¯é ç³»ç»Ÿçš„å…³é”®
- MLåŸºç¡€æ¦‚å¿µ (embedding) è¿æ¥äº†ä¼ ç»Ÿå·¥ç¨‹å’ŒAIåº”ç”¨

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**:
1. ä»Šæ™šå°è¯•åœ¨Rustä»£ç ä¸­ä½¿ç”¨mutexä¿æŠ¤å…±äº«çŠ¶æ€
2. é˜…è¯»Flink checkpointæœºåˆ¶ç›¸å…³ä»£ç 
3. æŸ¥çœ‹Lanceå¦‚ä½•å­˜å‚¨embeddingå‘é‡

---

**ç»§ç»­åŠ æ²¹!** ğŸš€ æ¯å¤©10ä¸ªè¯,ä¸€ä¸ªæœˆå°±èƒ½æŒæ¡300ä¸ªæŠ€æœ¯æœ¯è¯­!åšæŒå°±æ˜¯èƒœåˆ©!